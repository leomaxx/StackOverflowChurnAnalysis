{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import patsy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, confusion_matrix, classification_report, make_scorer, fbeta_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset answer_time_series to include only top answerers (Avg 2 answers monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/top_answerer_3year.pkl', 'rb') as picklefile:\n",
    "    top_answerer_3year = pickle.load(picklefile)\n",
    "with open('./data/processed/user_basic.pkl', 'rb') as picklefile:\n",
    "    user_basic = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment time series into 18 months, ignoring numbers before user signup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_answerer_3year.drop(['changes', 'change_history'], axis=1, inplace=True)\n",
    "user_basic.set_index('id', inplace=True)\n",
    "top_answerer_ts_creation = pd.merge(top_answerer_3year, user_basic[['creation_year', 'creation_month']], how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into many rows with 12 months' record, use the last n months to determine if user has churned\n",
    "def monthsSince2015(signup_year, signup_month):\n",
    "    baseline_date = datetime(2015, 1, 1, 00, 00)\n",
    "    return (signup_year-2015)*12 + (signup_month-baseline_date.month)\n",
    "def build_timeseries(df_ts, mode='convertEndStatus'):\n",
    "    df = deepcopy(df_ts)\n",
    "    ts_colname = ['M', 'M+1', 'M+2', 'M+3',\n",
    "             'M+4', 'M+5', 'M+6', 'M+7',\n",
    "             'M+8', 'M+9', 'M+10', 'M+11',\n",
    "             'M+12', 'M+13', 'M+14', 'M+15',\n",
    "             'M+16', 'M+17']\n",
    "    df['cutoff'] = df.apply(lambda x:monthsSince2015(x[36], x[37]), axis=1)\n",
    "    long_list = df[df['cutoff']<=0].iloc[:,0:18]\n",
    "    long_list.columns=ts_colname\n",
    "    print ('round 0 - added', long_list.shape[0], 'rows')\n",
    "    for i in range(1, df.shape[1] - 19):\n",
    "        eligible_values = df[df['cutoff']<=i].iloc[:,i:i+18]\n",
    "        eligible_values.columns=ts_colname\n",
    "        long_list = pd.concat([long_list, eligible_values])\n",
    "        print ('round', i, '- added', eligible_values.shape[0], 'rows')\n",
    "    if (mode == 'convertEndStatus'):\n",
    "        long_list['inactive'] = long_list.apply(lambda x:1 if np.sum(x[-6:])==0 else 0, axis=1)\n",
    "        long_list.drop(['M+12', 'M+13', 'M+14', 'M+15', 'M+16', 'M+17'], inplace=True, axis=1)\n",
    "    elif (mode == 'countLast4moAnswer'):\n",
    "        long_list['last6Month'] = long_list.apply(lambda x:np.sum(x[-6:]), axis=1)\n",
    "        long_list.drop(['M+12', 'M+13', 'M+14', 'M+15', 'M+16', 'M+17'], inplace=True, axis=1)\n",
    "    return long_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0 - added 13025 rows\n",
      "round 1 - added 13300 rows\n",
      "round 2 - added 13587 rows\n",
      "round 3 - added 13920 rows\n",
      "round 4 - added 14184 rows\n",
      "round 5 - added 14491 rows\n",
      "round 6 - added 14784 rows\n",
      "round 7 - added 15094 rows\n",
      "round 8 - added 15376 rows\n",
      "round 9 - added 15681 rows\n",
      "round 10 - added 15924 rows\n",
      "round 11 - added 16192 rows\n",
      "round 12 - added 16537 rows\n",
      "round 13 - added 16866 rows\n",
      "round 14 - added 17183 rows\n",
      "round 15 - added 17511 rows\n",
      "round 16 - added 17838 rows\n",
      "round 17 - added 18139 rows\n",
      "round 18 - added 18446 rows\n",
      "round 19 - added 18446 rows\n",
      "CPU times: user 59.8 s, sys: 464 ms, total: 1min\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%time top_answerer = build_timeseries(top_answerer_ts_creation, 'convertEndStatus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/top_answerer.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(top_answerer, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and format the user demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/user_basic.pkl', 'rb') as picklefile:\n",
    "    user_basic = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>about_me_length</th>\n",
       "      <th>creation_year</th>\n",
       "      <th>creation_month</th>\n",
       "      <th>last_access_year</th>\n",
       "      <th>last_access_month</th>\n",
       "      <th>location</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>profile_image</th>\n",
       "      <th>website_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16399</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>501</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.google.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15351</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>Austria</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47721</td>\n",
       "      <td>43</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.fuzzylizard.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7984</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>Laval, Canada</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16954</td>\n",
       "      <td>562</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>Hastings, United Kingdom</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>http://dominicblackwell.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  about_me_length  creation_year  creation_month  last_access_year  \\\n",
       "0  16399                5           2008               9              2017   \n",
       "1  15351                0           2008               9              2013   \n",
       "2  47721               43           2008              12              2016   \n",
       "3   7984                0           2008               9              2018   \n",
       "4  16954              562           2008               9              2012   \n",
       "\n",
       "   last_access_month                  location  up_votes  down_votes  \\\n",
       "0                  1           Toronto, Canada       501          37   \n",
       "1                  1                   Austria         2           0   \n",
       "2                  1           Toronto, Canada        12           3   \n",
       "3                  1             Laval, Canada        94          17   \n",
       "4                  1  Hastings, United Kingdom        66           3   \n",
       "\n",
       "   profile_image                  website_url  \n",
       "0              1       http://www.google.com/  \n",
       "1              1                               \n",
       "2              1   http://www.fuzzylizard.com  \n",
       "3              1                               \n",
       "4              1  http://dominicblackwell.com  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_basic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_basic['location'] = user_basic.location.apply(lambda x:x.split(', ')[-1])\n",
    "website_count = pd.DataFrame(user_basic.website_url.value_counts())\n",
    "website_count.reset_index(inplace=True)\n",
    "personal_website = set(website_count[website_count['website_url']==1]['index'].values)\n",
    "user_basic['personal_website'] = user_basic['website_url'].apply(lambda x:1 if x in personal_website else 0)\n",
    "user_basic.set_index('id', inplace=True)\n",
    "user_basic.drop(['last_access_year', 'last_access_month', 'website_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/user_basic_processed.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(user_basic, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the user demographic data with answer time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/top_answerer.pkl', 'rb') as picklefile:\n",
    "    top_answerer = pickle.load(picklefile)\n",
    "with open('./data/processed/user_basic_processed.pkl', 'rb') as picklefile:\n",
    "    user_basic = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_basic.drop(['up_votes', 'down_votes'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_answerer_basic = pd.merge(user_basic, top_answerer, how='right', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalize X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/location_dict.pkl', 'rb') as picklefile:\n",
    "    location_dict = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['NDF', 'NDF', 'APAC', 'AM', 'EMEA', 'EMEA', 'EMEA', 'AM', 'APAC', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'AM', 'EMEA', 'EMEA', 'APAC', 'AM', 'EMEA', 'EMEA', 'APAC', 'EMEA', 'EMEA', 'EMEA', 'AM', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'APAC', 'APAC', 'APAC', 'APAC', 'AM', 'APAC', 'AM', 'APAC', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'APAC', 'AM', 'EMEA', 'EMEA', 'APAC', 'EMEA', 'EMEA', 'EMEA', 'AM', 'APAC', 'AM', 'AM', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'AM', 'APAC', 'AM', 'AM', 'APAC', 'EMEA', 'APAC', 'EMEA', 'AM', '', 'AM', 'EMEA', 'AM', 'EMEA', 'EMEA', 'EMEA', '', 'AM', 'AM', 'AM', 'EMEA', 'APAC', 'EMEA', 'EMEA', 'AM', 'EMEA', 'EMEA', 'AM', 'AM', 'AM', 'AM', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'EMEA', 'AM', 'AM', 'AM'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = top_answerer_basic['inactive']\n",
    "X = top_answerer_basic.drop('inactive', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4444, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummify location\n",
    "with open('./data/processed/location_dict.pkl', 'rb') as picklefile:\n",
    "    location_dict = pickle.load(picklefile)\n",
    "    \n",
    "def map_dummify_Locations(df, location_dict):\n",
    "    df['location'] = df.location.apply(lambda x:location_dict[x] if x in set(location_dict.keys()) else 'Others')\n",
    "    location_dummy = patsy.dmatrix('location', data=df, return_type='dataframe')\n",
    "    df = pd.concat([df, location_dummy], axis = 1)\n",
    "    df.drop('location', axis=1, inplace=True)\n",
    "    new_colnames = [item.replace('[','-') for item in list(df.columns)]\n",
    "    new_colnames = [item.replace(']','') for item in new_colnames]\n",
    "    df.columns = new_colnames\n",
    "    df.drop('Intercept', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train = map_dummify_Locations(X_train, location_dict)\n",
    "X_test = map_dummify_Locations(X_test, location_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/user_ts_X_train.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(X_train, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "ssX = StandardScaler()\n",
    "X_train_norm = ssX.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled, y_train_resampled = RandomUnderSampler(random_state=4444).fit_sample(X_train_norm, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchFiveModels(X, y):\n",
    "    models = [\n",
    "        ('knn', KNN),\n",
    "        ('logistic', LogisticRegression),\n",
    "        ('tree', DecisionTreeClassifier),\n",
    "        ('forest', RandomForestClassifier),\n",
    "        ('xgboost', XGBClassifier)\n",
    "    ]\n",
    "\n",
    "    param_choices = [\n",
    "        {\n",
    "            'n_neighbors': range(2,12)\n",
    "        },\n",
    "        {\n",
    "            'C': np.logspace(-3,6,12),\n",
    "            'penalty':['l1', 'l2']\n",
    "        },\n",
    "        {\n",
    "            'max_depth': [2,3,4,5],\n",
    "            'min_samples_leaf': [3,6,10]\n",
    "        },\n",
    "        {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [1,2,3,4,5],\n",
    "            'min_samples_leaf': [3,6,10]\n",
    "        },\n",
    "        {\n",
    "            'max_depth': [3,4,5],\n",
    "            'n_estimators': [1, 50, 100,200],\n",
    "            'objective':['binary:logistic']\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    grids = {}\n",
    "    #ftwo_scorer = make_scorer(fbeta_score, average='binary', beta=1.41)\n",
    "    \n",
    "    for model_info, params in zip(models, param_choices):\n",
    "        print ('Now Fitting', model_info, '\\n')\n",
    "        name, model = model_info\n",
    "        grid = RandomizedSearchCV(model(), params, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "        s = \"{}: best score: {}\".format(name, grid.best_score_)\n",
    "        print(s)\n",
    "        grids[name] = grid\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Fitting ('knn', <class 'sklearn.neighbors.classification.KNeighborsClassifier'>) \n",
      "\n",
      "knn: best score: 0.7794393876540429\n",
      "Now Fitting ('logistic', <class 'sklearn.linear_model.logistic.LogisticRegression'>) \n",
      "\n",
      "logistic: best score: 0.7926119111453442\n",
      "Now Fitting ('tree', <class 'sklearn.tree.tree.DecisionTreeClassifier'>) \n",
      "\n",
      "tree: best score: 0.8363590972722968\n",
      "Now Fitting ('forest', <class 'sklearn.ensemble.forest.RandomForestClassifier'>) \n",
      "\n",
      "forest: best score: 0.8429240146827498\n",
      "Now Fitting ('xgboost', <class 'xgboost.sklearn.XGBClassifier'>) \n",
      "\n",
      "xgboost: best score: 0.8522513970117119\n"
     ]
    }
   ],
   "source": [
    "grid_clas = gridSearchFiveModels(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.81      0.80      4883\n",
      "          1       0.81      0.80      0.81      5117\n",
      "\n",
      "avg / total       0.80      0.80      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_subsample, grid_SMOTE['tree'].best_estimator_.predict(X_train_subsample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.81      0.87    107335\n",
      "          1       0.39      0.67      0.49     19040\n",
      "\n",
      "avg / total       0.85      0.79      0.81    126375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, grid_SMOTE['tree'].best_estimator_.predict(X_train_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87266, 20069],\n",
       "       [ 6227, 12813]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, grid_SMOTE['tree'].best_estimator_.predict(X_train_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
